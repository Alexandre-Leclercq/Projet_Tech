{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c075bf",
   "metadata": {},
   "source": [
    "# 1. Passive learning sans politique aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154fdaff",
   "metadata": {},
   "source": [
    "Nous initions le facteur de réduction $\\gamma = 0,95$ (proche de 1) dû au fait que nous considérons que les futures récompenses sont très importantes. Cependant, nous n'initialisons pas $\\gamma = 1$ car nous voulons pénalisé les états qui sont loins de la solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import SimpleMaze\n",
    "from agent import PassiveAgentTD\n",
    "\n",
    "row, col = 7, 12\n",
    "\n",
    "env = SimpleMaze(row, col)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5ad59",
   "metadata": {},
   "source": [
    "### Nous affichons l'évolution de l'utilité de chaque état : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdcbfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = PassiveAgentTD(env, gamma=0.95, debug=False)\n",
    "agent.learning(trials=100)\n",
    "agent.print_u_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "utilities = agent.get_utilities()\n",
    "visited_states = agent.get_visited_state()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "for i in range(len(visited_states)):\n",
    "    plt.plot(utilities[i], label=\"state \"+str(visited_states[i]))\n",
    "\n",
    "# Shrink current axis by 20%\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Number of trials\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77dec4d",
   "metadata": {},
   "source": [
    "### Nous affichons l'utilité finale pour chaque état où passe notre agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = agent.get_visited_state()\n",
    "utilities = agent.get_utilities()\n",
    "print(\"   \", end=\"\")\n",
    "for i in range(col):\n",
    "    print(\"{:<8}\".format(str(i)), end=\" \")\n",
    "print()\n",
    "for i in range(row):\n",
    "    print(\"{:<2}\".format(str(i)), end=\" \")\n",
    "    for j in range(col):\n",
    "        if [i, j] in states:\n",
    "            print(\"{:<8}\".format(str(round(utilities[states.index([i, j])][-1], 2))), end=\" \")\n",
    "        else:\n",
    "            print(\"{:<8}\".format(\"\"), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d306c6",
   "metadata": {},
   "source": [
    "# 2. Passive learning avec politique aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c337149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import SimpleMaze\n",
    "from agent import PassiveAgentTD\n",
    "\n",
    "row, col = 7, 12\n",
    "\n",
    "env = SimpleMaze(row, col)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PassiveAgentTD(env, seed=0, gamma=0.95, random_policy=True, debug=False)\n",
    "agent.learning(trials=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9fa7b1",
   "metadata": {},
   "source": [
    "### Nous affichons l'évolution de l'utilité de chaque état : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = agent.get_visited_state()\n",
    "utilities = agent.get_utilities()\n",
    "print(\"   \", end=\"\")\n",
    "for i in range(col):\n",
    "    print(\"{:<8}\".format(str(i)), end=\" \")\n",
    "print()\n",
    "for i in range(row):\n",
    "    print(\"{:<2}\".format(str(i)), end=\" \")\n",
    "    for j in range(col):\n",
    "        if [i, j] in states:\n",
    "            print(\"{:<8}\".format(str(round(utilities[states.index([i, j])][-1], 2))), end=\" \")\n",
    "        else:\n",
    "            print(\"{:<8}\".format(\"\"), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5d4cc",
   "metadata": {},
   "source": [
    "# 3. Active learning avec l'utilisation de la Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Maze\n",
    "from agent import ActiveAgentQLearning\n",
    "\n",
    "row, col = 7, 10\n",
    "\n",
    "env = Maze(row, col, seed=1, ratio_obstacles=0.2,ratio_hole=0.2)\n",
    "env.render(\"gui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46059117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = ActiveAgentQLearning(env, q_min=1000, n_min=400, gamma=0.95, debug=False)\n",
    "agent.learning(trials=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc937e",
   "metadata": {},
   "source": [
    "### Affichage de la solution que l'agent choisit après son apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95993df",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.play(\"gui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2838510",
   "metadata": {},
   "source": [
    "# 3. Régression Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036e8149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0131323ded6949aaafd548c7e30c1f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=336, width=480)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from environment import Maze\n",
    "from agent import ActiveAgentRegressionLearning\n",
    "\n",
    "row, col = 7, 10\n",
    "\n",
    "env = Maze(row, col, seed=1, ratio_obstacles=0.2,ratio_hole=0.2)\n",
    "env.render(\"gui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d812aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |██████████████████████████████████------------------------------------------------------------------| 34.0% \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_132808\\3351142280.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActiveAgentRegressionLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\ING2\\Projet_Tech\\agent.py\u001b[0m in \u001b[0;36mlearning\u001b[1;34m(self, trials)\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[0ms_prime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone_stage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0ms_prime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_polynomial_normalize_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_learning_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone_stage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 \u001b[0mprintProgressBar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_trials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__trials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ING2\\Projet_Tech\\agent.py\u001b[0m in \u001b[0;36mq_learning_agent\u001b[1;34m(self, s_prime, reward_prime)\u001b[0m\n\u001b[0;32m    364\u001b[0m                                      \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__r\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__gamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__q_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__q_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m                                      \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__rand_argmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_exploration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__q_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__Nsa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms_prime_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ING2\\Projet_Tech\\agent.py\u001b[0m in \u001b[0;36mfunction_exploration\u001b[1;34m(self, q, n)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__n_min\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m                 \u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__q_min\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = ActiveAgentRegressionLearning(env, q_min=10000, n_min=100, gamma=0.95, debug=False)\n",
    "agent.learning(trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e5755",
   "metadata": {},
   "source": [
    "L'apprentissage ne se passe pas comme prévu, en effet, nous rencontrons un problème avec l'apprentissage de $Q_\\beta(s,a)$. Cela aura pour effet de calculer des valeurs de $Q_\\beta(s,a)$ proche de la réalité par conséquent l'agent va se coincer dans une boucle d'état."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea7fca",
   "metadata": {},
   "source": [
    "### Affichage de la solution que l'agent choisit après son apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92286519",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.play(\"gui\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
